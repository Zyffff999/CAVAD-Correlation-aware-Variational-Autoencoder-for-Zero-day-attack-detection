import torch
import torch.nn.functional as F
from typing import Union, Tuple

def compute_gmm_kl_stable(mu: torch.Tensor,
                          logvar: torch.Tensor,
                          kl_clamp_max: float = 1000.0) -> torch.Tensor:
    """
    Computes the KL divergence between the posterior q(z|x) and a GMM prior p(z).
    This version is numerically stable and adapted to use the specific GMM parameter
    names from your model: `gmm_logits`, `gmm_means`, and `gmm_logvars`.

    KL(q(z|x) || p(z)) = -H(q(z|x)) - E_q(z|x)[log p(z)]

    Args:
        mu: Mean of the posterior q(z|x). Shape: [B, D]
        logvar: Log-variance of the posterior q(z|x). Shape: [B, D]
        kl_clamp_max: The maximum value to clamp the KL divergence at per batch item.

    Returns:
        KL divergence for each item in the batch. Shape: [B]
    """
    # --- Get GMM prior parameters from the model instance (`self`) ---
    # Convert logits to probabilities and log-variances to variances
    gmm_weights = F.softmax(gmm_logits, dim=0)
    gmm_means = gmm_means
    gmm_vars = torch.exp(self.gmm_logvars)

    latent_dim = latent_dim
    epsilon = 1e-8

    # --- Expand dimensions for broadcasting ---
    # Posterior params q(z|x)
    mu_expanded = mu.unsqueeze(1)  # Shape: [B, 1, D]
    var_expanded = torch.exp(logvar).unsqueeze(1)  # Shape: [B, 1, D]

    # Prior params p(z)
    gmm_means_expanded = gmm_means.unsqueeze(0)  # Shape: [1, K, D]
    gmm_vars_expanded = gmm_vars.unsqueeze(0)  # Shape: [1, K, D]

    # --- Compute E_q(z|x)[log p(z)], the cross-entropy term ---
    diff = mu_expanded - gmm_means_expanded  # Shape: [B, K, D]
    total_var = var_expanded + gmm_vars_expanded + epsilon  # Shape: [B, K, D]

    log_2pi = torch.log(torch.tensor(2 * torch.pi, device=mu.device))
    log_det = torch.sum(torch.log(total_var), dim=2)  # Shape: [B, K]
    mahal_dist = torch.sum(diff.pow(2) / total_var, dim=2)  # Shape: [B, K]

    # Log probability of z being generated by each GMM component
    log_probs_per_component = -0.5 * (log_det + mahal_dist + latent_dim * log_2pi)  # Shape: [B, K]

    # Weight by GMM mixture weights using log-sum-exp for stability
    log_weights = torch.log(gmm_weights + epsilon)  # Shape: [K]
    log_probs = log_probs_per_component + log_weights  # Shape: [B, K]

    # Log-sum-exp trick to get log p(z)
    max_log_prob = torch.max(log_probs, dim=1, keepdim=True)[0]
    log_prob_gmm = max_log_prob.squeeze(1) + torch.log(
        torch.sum(torch.exp(log_probs - max_log_prob), dim=1) + epsilon
    )  # Shape: [B]

    # --- Compute H(q(z|x)), the entropy of the posterior ---
    entropy_q = 0.5 * torch.sum(1 + logvar + log_2pi, dim=1)  # Shape: [B]

    # --- Final KL Divergence ---
    # kl = entropy - cross_entropy
    kl_per_element = entropy_q - log_prob_gmm  # Shape: [B]

    # Clamp to prevent extreme values, making the 1000 value configurable
    return torch.clamp(kl_per_element, min=0.0, max=kl_clamp_max)


def vae_loss_full(recon_logits: torch.Tensor,
                  x: torch.Tensor,
                  mu: torch.Tensor,
                  latent_param: torch.Tensor,
                  training_mode: str,
                  kld_weight: float = 1.0,
                  free_bits: float = 0.0,
                  kl_clamp_max: float = 5000.0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Perfected VAE loss function, adapted to your specific model structure.

    Args:
        recon_logits: Decoder output. Shape: [B, Seq, Emb]
        x: Original data. Shape: [B, Seq, Emb]
        mu: Latent means. Shape: [B, D]
        latent_param: For 'diagonal' or 'gmm', this is logvar [B, D].
                      For 'correlated', this is the Cholesky factor L [B, D, D].
        training_mode: One of ['diagonal', 'correlated', 'gmm'].
        kld_weight: Weight for the KL divergence term.
        free_bits: Minimum KL divergence value to enforce (per batch element).
        kl_clamp_max: Maximum value to clamp the GMM KL divergence at.

    Returns:
        A tuple containing (total_loss, reconstruction_loss, kl_divergence_loss).
    """
    batch_size = x.shape[0]
    latent_dim = mu.shape[1]

    # 1. Reconstruction Loss
    recon_flat = recon_logits.view(batch_size, -1)
    x_flat = x.view(batch_size, -1)
    recon_loss = F.mse_loss(recon_flat, x_flat, reduction='mean')

    # 2. KL Divergence Loss (kld_unweighted is the per-element KL)
    if training_mode == 'diagonal':
        logvar = latent_param
        kld_unweighted = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)

    elif training_mode == 'correlated':
        # VAE with full-covariance posterior vs. standard normal prior
        L = latent_param  # The Cholesky factor

        # log|Σ| = 2 * Σ log(L_ii)
        log_det_sigma = 2 * torch.sum(torch.log(L.diagonal(dim1=-2, dim2=-1) + 1e-8), dim=1)

        # tr(Σ) = ||L||²_F (Frobenius norm squared)
        trace_sigma = torch.sum(L.pow(2), dim=(1, 2))

        # ||μ||²
        mu_sq_norm = torch.sum(mu.pow(2), dim=1)

        # KL Div = 0.5 * (tr(Σ) + ||μ||² - k - log|Σ|)
        kld_unweighted = 0.5 * (trace_sigma + mu_sq_norm - latent_dim - log_det_sigma)

    elif training_mode == 'gmm':
        # VAE with diagonal posterior vs. a GMM prior
        logvar = latent_param
        kld_unweighted = compute_gmm_kl_stable(mu, logvar, kl_clamp_max)

    else:
        raise ValueError(f"Unknown training_mode: '{training_mode}'. "
                         "Must be 'diagonal', 'correlated', or 'gmm'.")

    if free_bits > 0:
        free_bits_tensor = torch.full_like(kld_unweighted, free_bits)
        kld_unweighted = torch.max(kld_unweighted, free_bits_tensor)

    kld_loss = kld_unweighted.mean()

    # 4. Total Weighted Loss
    total_loss = recon_loss + (kld_weight * kld_loss)

    return total_loss, recon_loss, kld_loss


def vae_elbo(recon_logits: torch.Tensor,
             x: torch.Tensor,
             mu: torch.Tensor,
             latent_param: torch.Tensor,
             beta: float = 1.0) -> Tuple[torch.Tensor, dict]:
    """
    Compute the Evidence Lower Bound (ELBO) for VAE.
    This is an alternative formulation that can be useful for analysis.

    Args:
        recon_logits: Decoder output
        x: Original data
        mu: Latent means
        latent_param: Covariance parameters (L or logvar)
        beta: Beta parameter for beta-VAE (default=1.0 for standard VAE)

    Returns:
        negative_elbo: -ELBO (minimize this)
        components: Dictionary with individual loss components
    """
    total_loss, recon_loss, kld_loss = vae_loss_full(
        recon_logits, x, mu, latent_param, kld_weight=beta, free_bits=0.0
    )

    # ELBO = log p(x|z) - KL[q(z|x) || p(z)]
    # We minimize -ELBO = -log p(x|z) + KL[q(z|x) || p(z)]
    # Which equals reconstruction_loss + beta * kl_loss

    components = {
        'reconstruction': recon_loss.item(),
        'kl_divergence': kld_loss.item(),
        'beta': beta,
        'elbo': -(recon_loss + beta * kld_loss).item()
    }

    return total_loss, components


def compute_reconstruction_loss(recon_x: torch.Tensor,
                                x: torch.Tensor,
                                loss_type: str = 'mse') -> torch.Tensor:
    """
    Compute reconstruction loss with different options.

    Args:
        recon_x: Reconstructed data
        x: Original data
        loss_type: Type of loss ('mse', 'l1', 'bce')

    Returns:
        Reconstruction loss
    """
    batch_size = x.shape[0]

    if loss_type == 'mse':
        return F.mse_loss(recon_x.view(batch_size, -1),
                          x.view(batch_size, -1),
                          reduction='mean')
    elif loss_type == 'l1':
        return F.l1_loss(recon_x.view(batch_size, -1),
                         x.view(batch_size, -1),
                         reduction='mean')
    elif loss_type == 'bce':
        # For binary data
        return F.binary_cross_entropy(recon_x.view(batch_size, -1),
                                      x.view(batch_size, -1),
                                      reduction='mean')
    else:
        raise ValueError(f"Unknown loss type: {loss_type}")


def compute_kl_divergence(mu: torch.Tensor,
                          logvar: torch.Tensor = None,
                          L: torch.Tensor = None,
                          reduction: str = 'mean') -> torch.Tensor:
    """
    Compute KL divergence between q(z|x) and p(z) = N(0, I).
    Handles both diagonal and full covariance cases.

    Args:
        mu: Mean of q(z|x) [batch_size, latent_dim]
        logvar: Log variance for diagonal case [batch_size, latent_dim]
        L: Cholesky factor for full covariance case [batch_size, latent_dim, latent_dim]
        reduction: How to reduce the loss ('mean', 'sum', 'none')

    Returns:
        KL divergence loss
    """
    if logvar is not None and L is not None:
        raise ValueError("Provide either logvar or L, not both")

    if logvar is not None:
        # Diagonal case: KL[N(μ, diag(σ²)) || N(0, I)]
        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)
    elif L is not None:
        # Full covariance case: KL[N(μ, LL^T) || N(0, I)]
        batch_size, latent_dim = mu.shape

        # log|Σ| = 2 * Σ log(L_ii)
        log_det = 2 * torch.log(torch.diagonal(L, dim1=-2, dim2=-1) + 1e-8).sum(dim=1)

        # tr(Σ) = ||L||²_F
        trace = (L ** 2).sum(dim=(1, 2))

        # ||μ||²
        mu_norm = (mu ** 2).sum(dim=1)

        # KL = 0.5 * (tr(Σ) + ||μ||² - k - log|Σ|)
        kl = 0.5 * (trace + mu_norm - latent_dim - log_det)
    else:
        raise ValueError("Provide either logvar or L")

    if reduction == 'mean':
        return kl.mean()
    elif reduction == 'sum':
        return kl.sum()
    elif reduction == 'none':
        return kl
    else:
        raise ValueError(f"Unknown reduction: {reduction}")


def compute_mmd_loss(z: torch.Tensor,
                     kernel: str = 'gaussian',
                     kernel_mul: float = 2.0,
                     kernel_num: int = 5) -> torch.Tensor:
    """
    Compute Maximum Mean Discrepancy (MMD) loss as an alternative to KL divergence.
    MMD measures the distance between the latent distribution and the prior.

    Args:
        z: Latent samples [batch_size, latent_dim]
        kernel: Kernel type ('gaussian', 'laplace')
        kernel_mul: Kernel width multiplier
        kernel_num: Number of kernel widths to use

    Returns:
        MMD loss
    """
    batch_size = z.shape[0]

    # Sample from prior
    z_prior = torch.randn_like(z)

    # Compute pairwise distances
    def compute_kernel(x, y, kernel_type='gaussian'):
        xx = torch.matmul(x, x.t())
        yy = torch.matmul(y, y.t())
        xy = torch.matmul(x, y.t())

        rx = xx.diag().unsqueeze(0).expand_as(xx)
        ry = yy.diag().unsqueeze(0).expand_as(yy)

        dxx = rx.t() + rx - 2 * xx
        dyy = ry.t() + ry - 2 * yy
        dxy = rx.t() + ry - 2 * xy

        if kernel_type == 'gaussian':
            XX = torch.zeros_like(dxx)
            YY = torch.zeros_like(dyy)
            XY = torch.zeros_like(dxy)

            bandwidth_range = [0.2, 0.5, 0.9, 1.3]
            for bandwidth in bandwidth_range:
                XX += torch.exp(-0.5 * dxx / bandwidth)
                YY += torch.exp(-0.5 * dyy / bandwidth)
                XY += torch.exp(-0.5 * dxy / bandwidth)

            return XX, YY, XY
        else:
            raise ValueError(f"Unknown kernel: {kernel_type}")

    XX, YY, XY = compute_kernel(z, z_prior, kernel)

    # MMD = E[k(x,x')] - 2E[k(x,y)] + E[k(y,y')]
    mmd = XX.mean() - 2 * XY.mean() + YY.mean()

    return mmd


class AnnealedLoss:
    def __init__(self,
                 initial_weight: float,
                 final_weight: float,
                 warmup_epochs: int,
                 annealing_type: str = 'linear'):
        """
        Args:
            initial_weight: Starting weight
            final_weight: Final weight after warmup
            warmup_epochs: Number of epochs for warmup
            annealing_type: Type of annealing ('linear', 'cosine', 'exponential')
        """
        self.initial_weight = initial_weight
        self.final_weight = final_weight
        self.warmup_epochs = warmup_epochs
        self.annealing_type = annealing_type

    def get_weight(self, epoch: int) -> float:
        """Get weight for current epoch"""
        if epoch >= self.warmup_epochs:
            return self.final_weight

        progress = epoch / self.warmup_epochs

        if self.annealing_type == 'linear':
            weight = self.initial_weight + (self.final_weight - self.initial_weight) * progress
        elif self.annealing_type == 'cosine':
            weight = self.final_weight - (self.final_weight - self.initial_weight) * \
                     (1 + torch.cos(torch.tensor(progress * 3.14159))) / 2
        elif self.annealing_type == 'exponential':
            weight = self.initial_weight * (self.final_weight / self.initial_weight) ** progress
        else:
            raise ValueError(f"Unknown annealing type: {self.annealing_type}")

        return float(weight)